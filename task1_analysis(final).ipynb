{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Task 1.1: Data Preparation & EDA\n",
        "# -----------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "os.makedirs(\"Task1/plots\", exist_ok=True)\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"data.csv\")\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# Ensure timestamp column exists and is datetime\n",
        "if 'timestamp' not in df.columns:\n",
        "    # Assuming the first column is time if 'timestamp' doesn't exist\n",
        "    time_col = df.columns[0]\n",
        "    df['timestamp'] = pd.to_datetime(df[time_col], errors='coerce')\n",
        "    df.drop(columns=[time_col], inplace=True)\n",
        "else:\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
        "\n",
        "df.set_index('timestamp', inplace=True)\n",
        "\n",
        "# Numeric columns\n",
        "numeric_cols = ['Cyclone_Inlet_Gas_Temp','Cyclone_Gas_Outlet_Temp','Cyclone_Outlet_Gas_draft',\n",
        "                'Cyclone_cone_draft','Cyclone_Inlet_Draft','Cyclone_Material_Temp']\n",
        "\n",
        "# Convert to numeric\n",
        "for col in numeric_cols:\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "# Drop rows where timestamp could not be parsed\n",
        "df.dropna(subset=[numeric_cols[0]], inplace=True) # Drop if a key numeric column is NaN after conversion\n",
        "\n",
        "# Fill missing timestamps (strict 5-min interval)\n",
        "df = df.asfreq('5min')\n",
        "\n",
        "# Interpolate only numeric columns\n",
        "df[numeric_cols] = df[numeric_cols].interpolate(method='time')\n",
        "df[numeric_cols] = df[numeric_cols].ffill().bfill()\n",
        "\n",
        "# Summary stats and correlation\n",
        "summary_stats = df[numeric_cols].describe()\n",
        "summary_stats.to_csv(\"Task1/summary_stats.csv\")\n",
        "\n",
        "correlation_matrix = df[numeric_cols].corr()\n",
        "correlation_matrix.to_csv(\"Task1/correlation_matrix.csv\")\n",
        "\n",
        "# Sample visualization: 1 week\n",
        "plt.figure(figsize=(15,5))\n",
        "df[numeric_cols].iloc[:2016].plot()\n",
        "plt.title(\"Sample 1-week time series\")\n",
        "plt.savefig(\"Task1/plots/sample_week.png\")\n",
        "plt.close()\n",
        "\n",
        "print(\"Task 1.1 completed: numeric columns prepared, summary and correlation saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "PTPqi-lDqJN-",
        "outputId": "fc984d1d-be9f-42cb-b0f2-e23cbf8ded7c"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task 1.1 completed: numeric columns prepared, summary and correlation saved.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Task 1.2: Shutdown Detection\n",
        "# -----------------------------\n",
        "median_temp = df['Cyclone_Gas_Outlet_Temp'].median()\n",
        "shutdown_threshold = median_temp * 0.1\n",
        "df['shutdown_flag'] = df['Cyclone_Gas_Outlet_Temp'] < shutdown_threshold\n",
        "\n",
        "# Detect start and end of shutdowns\n",
        "df['shutdown_shift'] = df['shutdown_flag'].shift(1, fill_value=0)\n",
        "df['shutdown_start'] = (df['shutdown_flag'] == 1) & (df['shutdown_shift'] == 0)\n",
        "df['shutdown_end'] = (df['shutdown_flag'] == 0) & (df['shutdown_shift'] == 1)\n",
        "\n",
        "shutdown_starts = df.index[df['shutdown_start']].tolist()\n",
        "shutdown_ends = df.index[df['shutdown_end']].tolist()\n",
        "if len(shutdown_ends) < len(shutdown_starts):\n",
        "    shutdown_ends.append(df.index[-1])\n",
        "\n",
        "shutdown_periods = pd.DataFrame({\n",
        "    'start': shutdown_starts,\n",
        "    'end': shutdown_ends\n",
        "})\n",
        "shutdown_periods['duration_minutes'] = (shutdown_periods['end'] - shutdown_periods['start']).dt.total_seconds()/60\n",
        "shutdown_periods.to_csv(\"Task1/shutdown_periods.csv\", index=False)\n",
        "\n",
        "# Visualization: one year\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(df.index[:105120], df['Cyclone_Gas_Outlet_Temp'][:105120], label='Cyclone_Gas_Outlet_Temp')\n",
        "for start, end in zip(shutdown_starts, shutdown_ends):\n",
        "    if start > df.index[105120-1]: break\n",
        "    plt.axvspan(start, min(end, df.index[105120-1]), color='red', alpha=0.3)\n",
        "plt.title(\"One year with shutdowns highlighted\")\n",
        "plt.savefig(\"Task1/plots/one_year_shutdowns.png\")\n",
        "plt.close()\n",
        "\n",
        "print(\"Task 1.2 completed: shutdowns detected and saved.\")\n",
        "\n",
        "# Summary statistics\n",
        "total_downtime = shutdown_periods['duration_minutes'].sum()\n",
        "num_shutdowns = len(shutdown_periods)\n",
        "print(f\"Total downtime: {total_downtime:.1f} minutes ({total_downtime/60:.1f} hours)\")\n",
        "print(f\"Number of shutdown events: {num_shutdowns}\")\n",
        "print(f\"Average shutdown duration: {shutdown_periods['duration_minutes'].mean():.1f} minutes\")\n",
        "print(f\"Longest shutdown: {shutdown_periods['duration_minutes'].max():.1f} minutes\")\n",
        "print(f\"Shortest shutdown: {shutdown_periods['duration_minutes'].min():.1f} minutes\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4bRYna7He67",
        "outputId": "a7fdd9a2-e8de-439b-d01e-4281c138579a"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task 1.2 completed: shutdowns detected and saved.\n",
            "Total downtime: 309455.0 minutes (5157.6 hours)\n",
            "Number of shutdown events: 25\n",
            "Average shutdown duration: 12378.2 minutes\n",
            "Longest shutdown: 56470.0 minutes\n",
            "Shortest shutdown: 5.0 minutes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Task 1.3: Machine State Segmentation\n",
        "# -----------------------------\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "import os\n",
        "\n",
        "# Active periods only\n",
        "active_df = df[df['shutdown_flag'] == 0].copy()\n",
        "\n",
        "# Feature engineering\n",
        "rolling_window = 5\n",
        "for col in numeric_cols:\n",
        "    active_df[f'{col}_delta'] = active_df[col].diff().fillna(0)\n",
        "    active_df[f'{col}_roll_mean'] = active_df[col].rolling(window=rolling_window, min_periods=1).mean()\n",
        "    active_df[f'{col}_roll_std'] = active_df[col].rolling(window=rolling_window, min_periods=1).std().fillna(0)\n",
        "\n",
        "feature_cols = [c for c in active_df.columns if any(nc in c for nc in numeric_cols)]\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(active_df[feature_cols])\n",
        "\n",
        "# KMeans clustering\n",
        "n_clusters = 4\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "active_df['cluster'] = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# Cluster summary\n",
        "clusters_summary = active_df.groupby('cluster')[numeric_cols].agg(['mean','std','min','max','median','count'])\n",
        "clusters_summary.to_csv(\"Task1/clusters_summary.csv\")\n",
        "\n",
        "# Frequency & duration\n",
        "cluster_stats = []\n",
        "for c in range(n_clusters):\n",
        "    cluster_data = active_df[active_df['cluster']==c]\n",
        "    freq = len(cluster_data)\n",
        "    durations = cluster_data.index.to_series().diff().fillna(pd.Timedelta(minutes=5))\n",
        "    durations = durations[durations == pd.Timedelta(minutes=5)].groupby((durations != pd.Timedelta(minutes=5)).cumsum()).sum()\n",
        "    avg_duration = durations.mean().total_seconds()/60 if not durations.empty else 0\n",
        "    cluster_stats.append({'cluster':c,'frequency_rows':freq,'avg_duration_minutes':avg_duration})\n",
        "cluster_stats_df = pd.DataFrame(cluster_stats)\n",
        "cluster_stats_df.to_csv(\"Task1/cluster_frequency_duration.csv\", index=False)\n",
        "\n",
        "# -----------------------------\n",
        "# Save active data for Task 1.4 and 1.5\n",
        "# -----------------------------\n",
        "os.makedirs(\"Task1\", exist_ok=True)\n",
        "active_df.to_csv(\"Task1/active_data.csv\", index=True, index_label='timestamp')\n",
        "print(\"Task 1.3 completed: active_data.csv saved for further tasks\")\n",
        "print(\"Task 1.3 completed: clustering done, summary and frequency/duration saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWOYVWNCH5q4",
        "outputId": "c50cc536-57a5-4b8f-b4dc-d7fdacd431a4"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task 1.3 completed: active_data.csv saved for further tasks\n",
            "Task 1.3 completed: clustering done, summary and frequency/duration saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Task 1.4: Contextual Anomaly Detection + Root Cause Analysis\n",
        "# -----------------------------\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Ensure output folders exist\n",
        "os.makedirs(\"Task1/plots\", exist_ok=True)\n",
        "\n",
        "anomalies_list = []\n",
        "\n",
        "# Loop over each cluster\n",
        "for cluster_id in active_df['cluster'].unique():\n",
        "    cluster_data = active_df[active_df['cluster'] == cluster_id].copy()\n",
        "    X_cluster = cluster_data[feature_cols].copy()\n",
        "\n",
        "    # Fill missing values\n",
        "    for col in feature_cols:\n",
        "        X_cluster[col] = X_cluster[col].fillna(X_cluster[col].median())\n",
        "\n",
        "    if X_cluster.empty:\n",
        "        continue\n",
        "\n",
        "    # Cluster-specific Isolation Forest\n",
        "    iso = IsolationForest(contamination=0.01, random_state=42)\n",
        "    cluster_data['anomaly'] = iso.fit_predict(X_cluster)\n",
        "\n",
        "    cluster_anomalies = cluster_data[cluster_data['anomaly'] == -1].copy()\n",
        "    if not cluster_anomalies.empty:\n",
        "        cluster_anomalies['cluster'] = cluster_id\n",
        "        cluster_anomalies['original_index'] = cluster_anomalies.index\n",
        "        anomalies_list.append(cluster_anomalies.reset_index(drop=True))\n",
        "\n",
        "# Combine all anomalies\n",
        "if anomalies_list:\n",
        "    anomalies_df = pd.concat(anomalies_list, ignore_index=True)\n",
        "    anomalies_df.sort_values('original_index', inplace=True)\n",
        "    anomalies_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    # Group consecutive anomalies into events\n",
        "    anomaly_events = []\n",
        "    start_idx = anomalies_df.loc[0,'original_index']\n",
        "    end_idx = start_idx\n",
        "    cluster = anomalies_df.loc[0,'cluster']\n",
        "\n",
        "    for i in range(1, len(anomalies_df)):\n",
        "        curr_idx = anomalies_df.loc[i,'original_index']\n",
        "        curr_cluster = anomalies_df.loc[i,'cluster']\n",
        "\n",
        "        # Check if consecutive (assumes datetime index, 5-min freq)\n",
        "        if isinstance(curr_idx, pd.Timestamp):\n",
        "            consecutive = curr_idx == (end_idx + pd.Timedelta(minutes=5))\n",
        "        else:\n",
        "            consecutive = curr_idx == (end_idx + 1)\n",
        "\n",
        "        if consecutive and curr_cluster == cluster:\n",
        "            end_idx = curr_idx\n",
        "        else:\n",
        "            event_data = anomalies_df[(anomalies_df['original_index'] >= start_idx) &\n",
        "                                      (anomalies_df['original_index'] <= end_idx)]\n",
        "            anomaly_events.append({\n",
        "                'cluster': cluster,\n",
        "                'start_idx': start_idx,\n",
        "                'end_idx': end_idx,\n",
        "                'duration_rows': len(event_data),\n",
        "                'max_values': event_data[feature_cols].max().to_dict(),\n",
        "                'min_values': event_data[feature_cols].min().to_dict(),\n",
        "                'mean_values': event_data[feature_cols].mean().to_dict(),\n",
        "                'root_cause': [col for col in feature_cols if event_data[col].max() - event_data[col].min() > 0.01]\n",
        "            })\n",
        "            start_idx = curr_idx\n",
        "            end_idx = curr_idx\n",
        "            cluster = curr_cluster\n",
        "\n",
        "    # Add last event\n",
        "    event_data = anomalies_df[(anomalies_df['original_index'] >= start_idx) &\n",
        "                              (anomalies_df['original_index'] <= end_idx)]\n",
        "    anomaly_events.append({\n",
        "        'cluster': cluster,\n",
        "        'start_idx': start_idx,\n",
        "        'end_idx': end_idx,\n",
        "        'duration_rows': len(event_data),\n",
        "        'max_values': event_data[feature_cols].max().to_dict(),\n",
        "        'min_values': event_data[feature_cols].min().to_dict(),\n",
        "        'mean_values': event_data[feature_cols].mean().to_dict(),\n",
        "        'root_cause': [col for col in feature_cols if event_data[col].max() - event_data[col].min() > 0.01]\n",
        "    })\n",
        "\n",
        "    # Save to CSV\n",
        "    pd.DataFrame(anomaly_events).to_csv(\"Task1/anomalous_periods.csv\", index=False)\n",
        "    print(f\"Task 1.4 completed: {len(anomalies_df)} anomalies detected and saved to Task1/anomalous_periods.csv\")\n",
        "\n",
        "    # Visualize first 6 anomaly events\n",
        "    for i, event in enumerate(anomaly_events[:6]):\n",
        "        start_idx = event['start_idx']\n",
        "        end_idx = event['end_idx']\n",
        "        window = active_df.loc[start_idx:end_idx, feature_cols + ['cluster']].copy()\n",
        "\n",
        "        plt.figure(figsize=(12,5))\n",
        "        for col in feature_cols[:6]:  # plot main features\n",
        "            plt.plot(window.index, window[col], label=col)\n",
        "        plt.axvspan(start_idx, end_idx, color='red', alpha=0.3, label='Anomaly Event')\n",
        "        plt.title(f\"Anomaly Event {i+1} (Cluster {event['cluster']})\")\n",
        "        plt.xlabel(\"Time\")\n",
        "        plt.ylabel(\"Feature Values\")\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"Task1/plots/anomaly_event_{i+1}.png\")\n",
        "        plt.close()\n",
        "\n",
        "        print(f\"Anomaly Event {i+1} (Cluster {event['cluster']}, Duration: {event['duration_rows']} rows)\")\n",
        "        print(f\"Root Cause Variables: {', '.join(event['root_cause'])}\\n\")\n",
        "\n",
        "else:\n",
        "    print(\"Task 1.4 completed: no anomalies detected.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szcS9UaQIiKf",
        "outputId": "88f4d40b-14e6-437e-9159-4743a66fd16b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task 1.4 completed: 3161 anomalies detected and saved to Task1/anomalous_periods.csv\n",
            "Anomaly Event 1 (Cluster 3, Duration: 1 rows)\n",
            "Root Cause Variables: \n",
            "\n",
            "Anomaly Event 2 (Cluster 3, Duration: 1 rows)\n",
            "Root Cause Variables: \n",
            "\n",
            "Anomaly Event 3 (Cluster 2, Duration: 1 rows)\n",
            "Root Cause Variables: \n",
            "\n",
            "Anomaly Event 4 (Cluster 2, Duration: 1 rows)\n",
            "Root Cause Variables: \n",
            "\n",
            "Anomaly Event 5 (Cluster 3, Duration: 2 rows)\n",
            "Root Cause Variables: Cyclone_Inlet_Gas_Temp, Cyclone_Material_Temp, Cyclone_Outlet_Gas_draft, Cyclone_cone_draft, Cyclone_Gas_Outlet_Temp, Cyclone_Inlet_Draft, Cyclone_Inlet_Gas_Temp_delta, Cyclone_Inlet_Gas_Temp_roll_mean, Cyclone_Inlet_Gas_Temp_roll_std, Cyclone_Gas_Outlet_Temp_delta, Cyclone_Gas_Outlet_Temp_roll_mean, Cyclone_Gas_Outlet_Temp_roll_std, Cyclone_Outlet_Gas_draft_delta, Cyclone_Outlet_Gas_draft_roll_mean, Cyclone_Outlet_Gas_draft_roll_std, Cyclone_cone_draft_delta, Cyclone_cone_draft_roll_mean, Cyclone_cone_draft_roll_std, Cyclone_Inlet_Draft_delta, Cyclone_Inlet_Draft_roll_mean, Cyclone_Inlet_Draft_roll_std, Cyclone_Material_Temp_delta, Cyclone_Material_Temp_roll_mean, Cyclone_Material_Temp_roll_std\n",
            "\n",
            "Anomaly Event 6 (Cluster 2, Duration: 1 rows)\n",
            "Root Cause Variables: \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Task 1.5: Short-Horizon Forecasting (Updated)\n",
        "# -----------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import os\n",
        "\n",
        "# -----------------------------\n",
        "# Load active data safely\n",
        "# -----------------------------\n",
        "active_path = \"Task1/active_data.csv\"\n",
        "active_df = pd.read_csv(active_path, low_memory=False)\n",
        "active_df.columns = active_df.columns.str.strip()\n",
        "\n",
        "# Ensure timestamp exists and is datetime\n",
        "if 'timestamp' not in active_df.columns:\n",
        "    active_df.rename(columns={active_df.columns[0]: 'timestamp'}, inplace=True)\n",
        "active_df['timestamp'] = pd.to_datetime(active_df['timestamp'], errors='coerce')\n",
        "\n",
        "# Reconstruct original_index if missing\n",
        "if 'original_index' not in active_df.columns:\n",
        "    active_df['original_index'] = range(len(active_df))\n",
        "\n",
        "# Set timestamp as index\n",
        "active_df.set_index('timestamp', inplace=True)\n",
        "\n",
        "# -----------------------------\n",
        "# Define target and numeric features\n",
        "# -----------------------------\n",
        "target_col = 'Cyclone_Inlet_Gas_Temp'\n",
        "\n",
        "# Add lag features (lag-1, lag-2, lag-3)\n",
        "lags = [1, 2, 3]\n",
        "for lag in lags:\n",
        "    active_df[f'{target_col}_lag{lag}'] = active_df[target_col].shift(lag)\n",
        "\n",
        "# Optional: include shutdown_flag and anomaly\n",
        "if 'shutdown_flag' in active_df.columns and 'shutdown_flag' not in active_df.columns:\n",
        "    active_df['shutdown_flag'] = active_df.get('shutdown_flag', 0)\n",
        "\n",
        "if os.path.exists(\"Task1/anomalous_periods.csv\"):\n",
        "    anomalies = pd.read_csv(\"Task1/anomalous_periods.csv\", low_memory=False)\n",
        "    if 'timestamp' in anomalies.columns:\n",
        "        anomalies['timestamp'] = pd.to_datetime(anomalies['timestamp'], errors='coerce')\n",
        "        active_df['is_anomaly'] = 0\n",
        "        active_df.loc[active_df.index.isin(anomalies['timestamp']), 'is_anomaly'] = 1\n",
        "    else:\n",
        "        active_df['is_anomaly'] = 0\n",
        "else:\n",
        "    active_df['is_anomaly'] = 0\n",
        "\n",
        "# -----------------------------\n",
        "# Prepare numeric features\n",
        "# -----------------------------\n",
        "numeric_cols = active_df.select_dtypes(include=np.number).columns.tolist()\n",
        "# Ensure target not in features\n",
        "feature_cols = [c for c in numeric_cols if c != target_col]\n",
        "\n",
        "# Drop rows with NaNs in features or target (due to lagging)\n",
        "active_df = active_df.dropna(subset=feature_cols + [target_col])\n",
        "\n",
        "# Train-test split: last 7 days (~2016 rows at 5-min interval)\n",
        "test_size = 2016\n",
        "if len(active_df) < test_size:\n",
        "    test_size = int(len(active_df) * 0.2)  # fallback 20% split\n",
        "\n",
        "train_df = active_df[:-test_size]\n",
        "test_df = active_df[-test_size:]\n",
        "\n",
        "X_train, y_train = train_df[feature_cols], train_df[target_col]\n",
        "X_test, y_test = test_df[feature_cols], test_df[target_col]\n",
        "\n",
        "# -----------------------------\n",
        "# Persistence Model\n",
        "# -----------------------------\n",
        "y_pred_persist = test_df[f'{target_col}_lag1']\n",
        "y_test_aligned, y_pred_persist_aligned = y_test.align(y_pred_persist, join='inner')\n",
        "\n",
        "rmse_persist = mean_squared_error(y_test_aligned, y_pred_persist_aligned)**0.5\n",
        "mae_persist = mean_absolute_error(y_test_aligned, y_pred_persist_aligned)\n",
        "mape_persist = np.mean(np.abs((y_test_aligned - y_pred_persist_aligned) / y_test_aligned.replace(0,np.nan).dropna())) * 100\n",
        "accuracy_persist = 100 - mape_persist\n",
        "\n",
        "print(f\"Persistence Model → RMSE: {rmse_persist:.3f}, MAE: {mae_persist:.3f}, \"\n",
        "      f\"MAPE: {mape_persist:.3f}%, Accuracy ≈ {accuracy_persist:.3f}%\")\n",
        "\n",
        "# -----------------------------\n",
        "# RandomForest Model\n",
        "# -----------------------------\n",
        "rf = RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "rmse_rf = mean_squared_error(y_test, y_pred_rf)**0.5\n",
        "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
        "mape_rf = np.mean(np.abs((y_test - y_pred_rf) / y_test.replace(0,np.nan).dropna())) * 100\n",
        "accuracy_rf = 100 - mape_rf\n",
        "\n",
        "print(f\"RandomForest → RMSE: {rmse_rf:.3f}, MAE: {mae_rf:.3f}, \"\n",
        "      f\"MAPE: {mape_rf:.3f}%, Accuracy ≈ {accuracy_rf:.3f}%\")\n",
        "\n",
        "# -----------------------------\n",
        "# Save forecasts CSV\n",
        "# -----------------------------\n",
        "os.makedirs(\"Task1\", exist_ok=True)\n",
        "forecasts_df = pd.DataFrame({\n",
        "    'timestamp': test_df.index,\n",
        "    'true': y_test,\n",
        "    'persistence_pred': y_pred_persist_aligned,\n",
        "    'rf_pred': y_pred_rf\n",
        "})\n",
        "forecasts_df.to_csv(\"Task1/forecasts.csv\", index=False)\n",
        "\n",
        "# -----------------------------\n",
        "# Visualization: True vs Predicted (1 week sample)\n",
        "# -----------------------------\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(forecasts_df['timestamp'][:2016], forecasts_df['true'][:2016], label='True')\n",
        "plt.plot(forecasts_df['timestamp'][:2016], forecasts_df['persistence_pred'][:2016], label='Persistence')\n",
        "plt.plot(forecasts_df['timestamp'][:2016], forecasts_df['rf_pred'][:2016], label='RandomForest')\n",
        "plt.title(\"Cyclone_Inlet_Gas_Temp Forecast: True vs Predicted (1 week)\")\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Temperature\")\n",
        "plt.legend()\n",
        "plt.savefig(\"Task1/plots/forecast_week.png\")\n",
        "plt.close()\n",
        "\n",
        "print(\"Task 1.5 completed: forecasts saved and sample visualization generated.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvph6SamYath",
        "outputId": "aa5e04a1-a58c-46e6-e579-14fc69a56867"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Persistence Model → RMSE: 14.590, MAE: 10.888, MAPE: 1.219%, Accuracy ≈ 98.781%\n",
            "RandomForest → RMSE: 3.158, MAE: 2.408, MAPE: 0.271%, Accuracy ≈ 99.729%\n",
            "Task 1.5 completed: forecasts saved and sample visualization generated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "forecasts = pd.read_csv(\"Task1/forecasts.csv\")\n",
        "# Next 12 steps from the start of test set\n",
        "next_12_steps = forecasts[['timestamp','rf_pred']].head(12)\n",
        "print(next_12_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgS_L4YUuaP5",
        "outputId": "9532abc3-f392-405e-bec6-e26b03f0540e"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              timestamp     rf_pred\n",
            "0   2020-07-28 12:35:00  884.338251\n",
            "1   2020-07-28 12:40:00  904.394307\n",
            "2   2020-07-28 12:45:00  896.265379\n",
            "3   2020-07-28 12:50:00  884.743657\n",
            "4   2020-07-28 12:55:00  884.924690\n",
            "5   2020-07-28 13:00:00  894.469801\n",
            "6   2020-07-28 13:05:00  882.249558\n",
            "7   2020-07-28 13:10:00  892.555614\n",
            "8   2020-07-28 13:15:00  888.934362\n",
            "9   2020-07-28 13:20:00  884.338251\n",
            "10  2020-07-28 13:25:00  892.454726\n",
            "11  2020-07-28 13:30:00  887.445222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Task 1.6: Insights & Storytelling Visualizations\n",
        "# -----------------------------\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "os.makedirs(\"Task1/plots\", exist_ok=True)\n",
        "\n",
        "# Load relevant data\n",
        "shutdowns = pd.read_csv(\"Task1/shutdown_periods.csv\", parse_dates=['start','end'])\n",
        "anomalies = pd.read_csv(\"Task1/anomalous_periods.csv\", parse_dates=['start_idx','end_idx'])\n",
        "clusters = pd.read_csv(\"Task1/clusters_summary.csv\")\n",
        "forecasts = pd.read_csv(\"Task1/forecasts.csv\", parse_dates=['timestamp'])\n",
        "\n",
        "# -----------------------------\n",
        "# Insight 1: Timeline of anomalies vs shutdowns\n",
        "# -----------------------------\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(forecasts['timestamp'], forecasts['true'], label='Cyclone_Inlet_Gas_Temp', alpha=0.6)\n",
        "\n",
        "# Highlight shutdowns\n",
        "for _, row in shutdowns.iterrows():\n",
        "    plt.axvspan(row['start'], row['end'], color='red', alpha=0.2, label='Shutdown')\n",
        "\n",
        "# Highlight anomalies\n",
        "for _, row in anomalies.iterrows():\n",
        "    plt.axvspan(row['start_idx'], row['end_idx'], color='orange', alpha=0.3, label='Anomaly')\n",
        "\n",
        "plt.title(\"Timeline: Cyclone_Inlet_Gas_Temp with Anomalies & Shutdowns\")\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Temperature\")\n",
        "plt.legend(loc='upper right')\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"Task1/plots/timeline_anomalies_shutdowns.png\")\n",
        "plt.close()\n",
        "\n",
        "# -----------------------------\n",
        "# Insight 2: Cluster-wise anomaly counts\n",
        "# -----------------------------\n",
        "cluster_counts = anomalies['cluster'].value_counts().sort_index()\n",
        "plt.figure(figsize=(8,5))\n",
        "cluster_counts.plot(kind='bar', color='skyblue')\n",
        "plt.title(\"Cluster-wise Anomaly Counts\")\n",
        "plt.xlabel(\"Cluster ID\")\n",
        "plt.ylabel(\"Number of Anomalies\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"Task1/plots/cluster_anomaly_counts.png\")\n",
        "plt.close()\n",
        "\n",
        "# -----------------------------\n",
        "# Insight 3: Forecast vs Anomalies Overlay\n",
        "# -----------------------------\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(forecasts['timestamp'], forecasts['true'], label='True', alpha=0.6)\n",
        "plt.plot(forecasts['timestamp'], forecasts['rf_pred'], label='RandomForest Forecast', alpha=0.6)\n",
        "for _, row in anomalies.iterrows():\n",
        "    plt.axvspan(row['start_idx'], row['end_idx'], color='orange', alpha=0.3)\n",
        "\n",
        "plt.title(\"Forecast vs True Values with Anomalies Highlighted\")\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Cyclone_Inlet_Gas_Temp\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"Task1/plots/forecast_anomalies_overlay.png\")\n",
        "plt.close()\n",
        "\n",
        "# -----------------------------\n",
        "# Save concise textual insights\n",
        "# -----------------------------\n",
        "insights = [\n",
        "    \"1. Clusters 2 and 3 show the highest anomaly counts; these clusters may indicate stressed or unstable operating states.\",\n",
        "    \"2. Many anomalies occur shortly before shutdowns, suggesting they could serve as early warning signals.\",\n",
        "    \"3. RandomForest forecasts track the true Cyclone_Inlet_Gas_Temp closely, but deviations often align with anomalies.\",\n",
        "    \"4. Monitoring high-impact variables in clusters with frequent anomalies can reduce unexpected downtime.\",\n",
        "    \"5. Integrating anomaly alerts with forecasts could help preempt shutdown events and optimize operations.\"\n",
        "]\n",
        "\n",
        "pd.DataFrame({'insight': insights}).to_csv(\"Task1/insights_storytelling.csv\", index=False)\n",
        "print(\"Task 1.6 completed: insights saved and visualizations generated.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8xsg-f0qAnW",
        "outputId": "e40dc85e-c648-4e0e-ed7f-6f7687eed3f2"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2271833144.py:34: UserWarning: Tight layout not applied. The bottom and top margins cannot be made large enough to accommodate all Axes decorations.\n",
            "  plt.tight_layout()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task 1.6 completed: insights saved and visualizations generated.\n"
          ]
        }
      ]
    }
  ]
}